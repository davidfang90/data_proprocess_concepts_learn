{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. pre-process dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Read dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read the dictionary\n",
    "dic = []\n",
    "with open(\"dict.txt\",\"r\") as dic_fr:\n",
    "    dic = dic_fr.readlines()\n",
    "for i in range(len(dic)):\n",
    "    d = dic[i]\n",
    "    dic[i] = d.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 remove '-' and blank line and stopwords in dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_dict = []\n",
    "for i in range(len(dic)):\n",
    "    if '-' in dic[i]:# remove '-'\n",
    "        tmp_dict.append( dic[i].replace(\"-\",\" \") )\n",
    "    else:\n",
    "        tmp_dict.append( dic[i] )\n",
    "dic = tmp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_dict = []\n",
    "for i in range(len(dic)):\n",
    "    if '' != dic[i]:# remove ''\n",
    "        tmp_dict.append( dic[i] )\n",
    "dic = tmp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "import nltk\n",
    "english_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "tmp_dict = []\n",
    "for i in range(len(dic)):\n",
    "    words = nltk.word_tokenize(dic[i])\n",
    "    tmp = []\n",
    "    for w in words:\n",
    "        if w not in english_stopwords:\n",
    "            tmp.append(w)\n",
    "    tmp_dict.append(' '.join(tmp)) \n",
    "dic = tmp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 dic capital letter 2 lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(dic)):\n",
    "    dic_phrase_word_list = dic[i].split()\n",
    "    for i in range(len(dic_phrase_word_list)):\n",
    "        word = dic_phrase_word_list[i]\n",
    "        capital_letter_count = 0\n",
    "        for letter in word:\n",
    "            if not letter.islower():\n",
    "                capital_letter_count+=1\n",
    "        if capital_letter_count>2:\n",
    "            pass\n",
    "        else:\n",
    "            dic_phrase_word_list[i] = word.lower()\n",
    "    dic[i] = ' '.join(dic_phrase_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. pre-process text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Read text as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 276 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#read dict data\n",
    "text_abstract_dictdata = []\n",
    "text_title_dictdata = []\n",
    "with open (\"result_chrome_dict.txt\",'r',encoding = \"UTF-8\") as fr:\n",
    "    lines = fr.readlines()\n",
    "    for line in lines:\n",
    "        tmp = line.split(\"abstract:\")# abstract only\n",
    "        if(len(tmp)>1):\n",
    "            line_split = tmp[1]\n",
    "            line_abstract_split = line_split.strip()\n",
    "            text_abstract_dictdata.append(line_abstract_split)\n",
    "            text_title_split_tmp = tmp[0].strip(\"title:\")\n",
    "            text_title_split = text_title_split_tmp.strip(\"===\")\n",
    "            text_title_dictdata.append(text_title_split)\n",
    "            #text_title = text_title.append(text_title.strip(\"title:\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#dic data --->title+abstract\n",
    "text_list_dictdata = []\n",
    "for i in range(len(text_title_dictdata)):\n",
    "    for j in range(len(text_abstract_dictdata)):\n",
    "        if i == j:\n",
    "            text_list_tmp = text_title_dictdata[i]+ \" \" + text_abstract_dictdata[j]\n",
    "            text_list_dictdata.append(text_list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read radar data\n",
    "import json\n",
    "import re\n",
    "with open('result_chrome_1.json','r',encoding = 'utf-8') as fr:\n",
    "    lines = fr.readlines()\n",
    "rep = re.compile(\"\\xA9.*\")\n",
    "\n",
    "result_abstract_radardata = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        line = json.loads(line)\n",
    "        abstract = rep.sub(\"\", line['abstract']).strip()\n",
    "        result_abstract_radardata.append(abstract)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "result_title_radardata = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        jdata = json.loads(line)\n",
    "        result_title_radardata.append(jdata['title'])\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#radar data --->title+abstract\n",
    "text_list_radardata = []\n",
    "for i in range(len(result_title_radardata)):\n",
    "    for j in range(len(result_abstract_radardata)):\n",
    "        if i == j:\n",
    "            text_tmp = result_title_radardata[i]+ \" \" + result_abstract_radardata[j]\n",
    "            text_list_radardata.append(text_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all data named text_list\n",
    "text_list = text_list_dictdata + text_list_radardata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43117"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Remove the repeated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39428"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = list(set(text_list))\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31853"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt = text_list\n",
    "tmp_txt = \" \".join(tmp_txt)\n",
    "count = 0\n",
    "for i in range(len(dic)):\n",
    "    text_split_by_phrase = tmp_txt.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Remove \"this paper\",\"in this paper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useless = [\"this paper\",\"in this paper\",\"This paper\",\"In this paper\", '[无可用摘要]','0','1','2','3','4','5','6','7','8','9','<','>','[',']','``',\"''\",'.',\"'\",'``','ghz','GHz',\"''\",]\n",
    "for i in range(len(useless)):\n",
    "    for j in range(len(text_list)):\n",
    "        if useless[i] in text_list[j]:\n",
    "            text_list[j] = text_list[j].replace(useless[i],\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31853"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt = text_list\n",
    "tmp_txt = \" \".join(tmp_txt)\n",
    "count = 0\n",
    "for i in range(len(dic)):\n",
    "    text_split_by_phrase = tmp_txt.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    #count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39428"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 remove part capital letter in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tmp_text_list = []\n",
    "for txt in text_list:\n",
    "    capital_letter_count = 0\n",
    "    sum_letter_count = 0\n",
    "    for a in txt:\n",
    "        if not a.islower():\n",
    "            capital_letter_count+=1\n",
    "        sum_letter_count+=1\n",
    "    if(capital_letter_count/sum_letter_count<0.9):\n",
    "        tmp_text_list.append(txt)\n",
    "text_list = tmp_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39387"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31853"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt = text_list\n",
    "tmp_txt = \" \".join(tmp_txt)\n",
    "count = 0\n",
    "for i in range(len(dic)):\n",
    "    text_split_by_phrase = tmp_txt.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    #count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ' '.join(text_list)#将文本变为一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45625993"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "word_list = nltk.word_tokenize(text)#分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7084581"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31853"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt = text_list\n",
    "tmp_txt = \" \".join(tmp_txt)\n",
    "count = 0\n",
    "for i in range(len(dic)):\n",
    "    text_split_by_phrase = tmp_txt.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    #count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 remove punctuation in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from string import punctuation\n",
    "tmp = []\n",
    "for item in word_list:\n",
    "    if (item not in punctuation):\n",
    "        tmp.append(item)        \n",
    "word_list = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6622496"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32381"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt = ' '.join(word_list)\n",
    "count = 0\n",
    "for i in range(len(dic)):\n",
    "    text_split_by_phrase = tmp_txt.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    #count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 remove '-' and blank line in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_txt = []\n",
    "for i in range(len(word_list)):\n",
    "    if '-' in word_list[i]:\n",
    "        tmp_txt.append( word_list[i].replace(\"-\",\" \") )\n",
    "    else:\n",
    "        tmp_txt.append( word_list[i] )\n",
    "word_list = tmp_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6622496"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34714"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt = ' '.join(word_list)\n",
    "count = 0\n",
    "for i in range(len(dic)):\n",
    "    text_split_by_phrase = tmp_txt.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    #count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 remove stop words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_txt = []\n",
    "english_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "for i in range(len(word_list)):\n",
    "    if word_list[i] not in english_stopwords:\n",
    "           tmp_txt.append(word_list[i]) \n",
    "word_list = tmp_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35291"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt = ' '.join(word_list)\n",
    "count = 0\n",
    "for i in range(len(dic)):\n",
    "    text_split_by_phrase = tmp_txt.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    #count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4420662"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 capital 2 lower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text \n",
    "for i in range(len(word_list)):\n",
    "    word = word_list[i]\n",
    "    capital_letter_count = 0\n",
    "    for letter in word:\n",
    "        if not letter.islower():\n",
    "            capital_letter_count+=1      \n",
    "    if capital_letter_count>2:\n",
    "        pass\n",
    "    else:\n",
    "        word_list[i] = word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4420662"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46487"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt = ' '.join(word_list)\n",
    "count = 0\n",
    "for i in range(len(dic)):\n",
    "    text_split_by_phrase = tmp_txt.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    #count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10.1 use snowball stemming methods in NLTK for our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_word_list_1 = word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# text stemming by EnglishStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "st = EnglishStemmer()\n",
    "\n",
    "for i in range(len(tmp_word_list_1)):\n",
    "    words = tmp_word_list_1[i].split(' ')\n",
    "    wordslist = []\n",
    "    for w in words:\n",
    "        wordslist.append(st.stem(w))\n",
    "    \n",
    "    tmp_word_list_1[i] = ' '.join(wordslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dic stemming by EnglishStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "st = EnglishStemmer()\n",
    "tmp_dic_1 = dic\n",
    "for i in range(len(tmp_dic_1)):\n",
    "    dic_phrase_word_list = tmp_dic_1[i].split(' ')\n",
    "    for j in range(len(dic_phrase_word_list)):\n",
    "        word = dic_phrase_word_list[j]\n",
    "        dic_phrase_word_list[j] = st.stem(word)\n",
    "    tmp_dic_1[i] = ' '.join(dic_phrase_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64741"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt_1 = ' '.join(tmp_word_list_1)\n",
    "count = 0\n",
    "for i in range(len(tmp_dic_1)):\n",
    "    text_split_by_phrase = tmp_txt_1.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    #count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10.2 use wordnet stemming methods in NLTK for our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_word_list_2 = word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#text stemming by WordNetLemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "st = WordNetLemmatizer()\n",
    "for i in range(len(tmp_word_list_2)):\n",
    "    words = tmp_word_list_2[i].split(' ')\n",
    "    wordslist = []\n",
    "    for w in words:\n",
    "        wordslist.append(st.lemmatize(w))\n",
    "    \n",
    "    tmp_word_list_2[i] = ' '.join(wordslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dic stemming by WordNetLemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "st = WordNetLemmatizer()\n",
    "tmp_dic_2 = dic\n",
    "for i in range(len(tmp_dic_2)):\n",
    "    dic_phrase_word_list = tmp_dic_2[i].split(' ')\n",
    "    for j in range(len(dic_phrase_word_list)):\n",
    "        word = dic_phrase_word_list[j]\n",
    "        dic_phrase_word_list[j] = st.lemmatize(word)\n",
    "    tmp_dic_2[i] = ' '.join(dic_phrase_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64712"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------test code---------------------\n",
    "tmp_txt_2 = ' '.join(tmp_word_list_2)\n",
    "count = 0\n",
    "for i in range(len(tmp_dic_2)):\n",
    "    text_split_by_phrase = tmp_txt_2.split(dic[i])\n",
    "    count += len(text_split_by_phrase)-1\n",
    "    #count_tmp = len(text_split_by_phrase)-1\n",
    "    #print('{0}: count={1}'.format(i, count_tmp))\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we choose the snowball stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Dump the dataset by pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(tmp_txt_1, open('txt_preprocessed.p', 'wb'))\n",
    "pickle.dump(tmp_dic_1, open('list_dic_preprocessed.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
